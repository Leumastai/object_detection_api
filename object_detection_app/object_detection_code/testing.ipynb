{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=/usr/local/lib64:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read image\n",
    "image_path = \"/home/samuel/Downloads/istockphoto-1166584467-612x612.jpg\"\n",
    "weights = \"/home/samuel/Documents/Bloverse/bloverse-projects/IDK/object_detection/object_detection_app/yolov3_files/yolov3.weights\"\n",
    "config = \"/home/samuel/Documents/Bloverse/bloverse-projects/IDK/object_detection/object_detection_app/yolov3_files/yolov3.cfg\"\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "width = image.shape[1]\n",
    "height = image.shape[0]\n",
    "scale = 0.00392\n",
    "\n",
    "#read class names from text file\n",
    "classes = \"/home/samuel/Documents/Bloverse/bloverse-projects/IDK/object_detection/object_detection_app/yolov3_files/yolov3.txt\"\n",
    "with open(classes, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Generate different BBox colors for different classes\n",
    "COLORS = np.random.uniform(\n",
    "    0, 255, size=(len(classes), 3))\n",
    "\n",
    "# Read pretrained model and config file\n",
    "net = cv2.dnn.readNet(weights, config)\n",
    "\n",
    "# Creatr input blob\n",
    "blob = cv2.dnn.blobFromImage(\n",
    "    image, scale, (416, 416), (0, 0, 0),\n",
    "    True, crop=False)\n",
    "\n",
    "# Set input blob for the network\n",
    "net.setInput(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layers(net):\n",
    "    # Get the output layer names in the achitecture\n",
    "\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "def draw_bounding_box(\n",
    "    img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "    # Function to draw bounding box on the detected object \n",
    "    # with the class name\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "    color = COLORS[class_id]\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w, y_plus_h), color, 2)\n",
    "    cv2.putText(\n",
    "        img, label, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.5, color, 2)\n",
    "\n",
    "# Run inference through the network and get predictions from output layer\n",
    "outs = net.forward(get_output_layers(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "conf_threshold = 0.7\n",
    "nums_threshold = 0.4\n",
    "\n",
    "# From each detection from the output layer get the \n",
    "# confidence, class_id, bounding_box params\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > conf_threshold:\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            x = center_x - w/2\n",
    "            y = center_y - h/2\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Non-max Suppresion\n",
    "indices = cv2.dnn.NMSBoxes(\n",
    "    boxes, confidences, conf_threshold, nums_threshold)\n",
    "\n",
    "labels_class = []\n",
    "labels_confidence = []\n",
    "labels_bbox = []\n",
    "for i in indices:\n",
    "    #i = i[0]\n",
    "    box = boxes[i]\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    \n",
    "    labels_confidence.append(confidences[i])\n",
    "    labels_bbox.append([x, y, w, h])\n",
    "    labels_class.append(str(classes[class_ids[i]]))\n",
    "    \n",
    "    draw_bounding_box(\n",
    "        image, class_ids[i], confidences[i], round(x),\n",
    "        round(y), round(x+w), round(y+h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "image = face_recognition.load_image_file(image_path)\n",
    "face_locations = face_recognition.face_locations(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(labels_class) != len(face_locations):    \n",
    "    # i.e there are some objects except from a person without face\n",
    "    diff = len(labels_class) - len(face_locations)\n",
    "    n = np.append([], np.repeat(\"Null\", diff))\n",
    "    face_locations = [*face_locations, *n]\n",
    "\n",
    "all_image_metadata = {}\n",
    "count = 1\n",
    "for i in zip(labels_class, labels_confidence, labels_bbox, face_locations):\n",
    "    label = i[0]\n",
    "    confidence = i[1]\n",
    "    object_bbox = i[2]\n",
    "    face_bbox = i[3]\n",
    "    image_metadata = {\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"object_bbox\": object_bbox,\n",
    "    }\n",
    "    face_metadata = {\n",
    "        \"face_bbox\": face_bbox\n",
    "    }\n",
    "    if face_bbox != np.nan:\n",
    "        all_image_metadata.update(\n",
    "            {\n",
    "                \"object_metadata_\" + str(count) : image_metadata,\n",
    "                \"face_dict\": face_metadata\n",
    "            })\n",
    "        count += 1\n",
    "    else:\n",
    "        all_image_metadata.update(\n",
    "            {\"object_metadata_\" + str(count) : image_metadata})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null\n",
      "Null\n"
     ]
    }
   ],
   "source": [
    "for i in face_locations:\n",
    "    if i == \"Null\":\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object_metadata_1': {'label': 'person',\n",
       "  'confidence': 0.9998630285263062,\n",
       "  'object_bbox': [262.5, 34.0, 219, 380]},\n",
       " 'face_dict': {'face_bbox': 'Null'},\n",
       " 'object_metadata_2': {'label': 'person',\n",
       "  'confidence': 0.9986932873725891,\n",
       "  'object_bbox': [95.0, 93.5, 194, 309]},\n",
       " 'object_metadata_3': {'label': 'cell phone',\n",
       "  'confidence': 0.9773884415626526,\n",
       "  'object_bbox': [351.0, 249.0, 32, 26]},\n",
       " 'object_metadata_4': {'label': 'cell phone',\n",
       "  'confidence': 0.7303444743156433,\n",
       "  'object_bbox': [161.0, 253.0, 34, 20]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_image_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(labels_class) != len(face_locations):    \n",
    "    # i.e there are some objects except from a person without face\n",
    "    diff = len(labels_class) - len(face_locations)\n",
    "    n = np.append([], np.repeat(np.nan, diff))\n",
    "    face_locations = [*face_locations, *n]\n",
    "\n",
    "count = 1\n",
    "all_image_metadata = {}\n",
    "for i in zip(labels_class, labels_confidence, labels_bbox, face_locations):\n",
    "    label = i[0]\n",
    "    confidence = i[1]\n",
    "    object_bbox = i[2]\n",
    "    face_bbox = i[3]\n",
    "\n",
    "    image_metadata = {\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"object_bbox\": object_bbox,\n",
    "        \"face_dict\": {\n",
    "            \"face_bbox\": face_bbox\n",
    "        }\n",
    "    }\n",
    "\n",
    "    all_image_metadata.update(\n",
    "        {\"object_metadata_\" + str(count) : image_metadata})\n",
    "\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object_metadata_1': {'label': 'person',\n",
       "  'confidence': 0.9998630285263062,\n",
       "  'object_bbox': [262.5, 34.0, 219, 380],\n",
       "  'face_dict': {'face_bbox': (129, 246, 191, 183)}},\n",
       " 'object_metadata_2': {'label': 'person',\n",
       "  'confidence': 0.9986932873725891,\n",
       "  'object_bbox': [95.0, 93.5, 194, 309],\n",
       "  'face_dict': {'face_bbox': (88, 370, 163, 295)}},\n",
       " 'object_metadata_3': {'label': 'cell phone',\n",
       "  'confidence': 0.9773884415626526,\n",
       "  'object_bbox': [351.0, 249.0, 32, 26],\n",
       "  'face_dict': {'face_bbox': nan}},\n",
       " 'object_metadata_4': {'label': 'cell phone',\n",
       "  'confidence': 0.7303444743156433,\n",
       "  'object_bbox': [161.0, 253.0, 34, 20],\n",
       "  'face_dict': {'face_bbox': nan}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_image_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'person', 'cell phone', 'cell phone']\n"
     ]
    }
   ],
   "source": [
    "{'type': 'primary_object',\n",
    "  'label': 'Person',\n",
    "  'confidence': 0.8641915321350098,\n",
    "  'object_bbox': [329, 79, 1032, 663],\n",
    "  'obj_perc': 0.45, # percentage of the image covered by the object\n",
    "  'face_dict': {'face_bbox': [585, 93, 814, 359],'face_perc': 0.07}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"obj_detect.jpg\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' face_loc = []\\nfor i in range(0, len(face_locations)):\\n    _, _, width, height= face_locations[i]\\n    # These number are choosen based on teh\\n    if (width < 1000) & (height < 1000):\\n        real_face = face_locations[i]\\n        top, right, buttom, left = real_face '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" face_loc = []\n",
    "for i in range(0, len(face_locations)):\n",
    "    _, _, width, height= face_locations[i]\n",
    "    # These number are choosen based on teh\n",
    "    if (width < 1000) & (height < 1000):\n",
    "        real_face = face_locations[i]\n",
    "        top, right, buttom, left = real_face \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(129, 246, 191, 183), (88, 370, 163, 295)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://192.168.43.159:5000/detect\"\n",
    "\n",
    "payload={}\n",
    "files=[\n",
    "  ('image',('istockphoto-1166584467-612x612.jpg',open('/home/samuel/Downloads/istockphoto-1166584467-612x612.jpg','rb'),'image/jpeg'))\n",
    "]\n",
    "headers = {}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248327"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO-V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants.\n",
    "INPUT_WIDTH = 640\n",
    "INPUT_HEIGHT = 640\n",
    "SCORE_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.45\n",
    "CONFIDENCE_THRESHOLD = 0.45\n",
    "# Text parameters.\n",
    "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 0.7\n",
    "THICKNESS = 1\n",
    "# Colors.\n",
    "BLACK  = (0,0,0)\n",
    "BLUE   = (255,178,50)\n",
    "YELLOW = (0,255,255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_label(im, label, x, y):\n",
    "    \"\"\"Draw text onto image at location.\"\"\"\n",
    "    # Get text size.\n",
    "    text_size = cv2.getTextSize(label, FONT_FACE, FONT_SCALE, THICKNESS)\n",
    "    dim, baseline = text_size[0], text_size[1]\n",
    "    # Use text size to create a BLACK rectangle.\n",
    "    cv2.rectangle(im, (x,y), (x + dim[0], y + dim[1] + baseline), (0,0,0), cv2.FILLED);\n",
    "    # Display text inside the rectangle.\n",
    "    cv2.putText(im, label, (x, y + dim[1]), FONT_FACE, FONT_SCALE, YELLOW, THICKNESS, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(input_image, net):\n",
    "      # Create a 4D blob from a frame.\n",
    "      blob = cv2.dnn.blobFromImage(input_image, 1/255,  (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
    "\n",
    "      # Sets the input to the network.\n",
    "      net.setInput(blob)\n",
    "\n",
    "      # Run the forward pass to get output of the output layers.\n",
    "      outputs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "      return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(input_image, outputs):\n",
    "    # Lists to hold respective values while unwrapping.\n",
    "    labels_class = []\n",
    "    labels_confidence = []\n",
    "    labels_bbox = []\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    # Rows.\n",
    "    rows = outputs[0].shape[1]\n",
    "    image_height, image_width = input_image.shape[:2]\n",
    "    # Resizing factor.\n",
    "    x_factor = image_width / INPUT_WIDTH\n",
    "    y_factor =  image_height / INPUT_HEIGHT\n",
    "    # Iterate through detections.\n",
    "    for r in range(rows):\n",
    "        row = outputs[0][0][r]\n",
    "        confidence = row[4]\n",
    "        # Discard bad detections and continue.\n",
    "        if confidence >= CONFIDENCE_THRESHOLD:\n",
    "              classes_scores = row[5:]\n",
    "              # Get the index of max class score.\n",
    "              class_id = np.argmax(classes_scores)\n",
    "              #  Continue if the class score is above threshold.\n",
    "              if (classes_scores[class_id] > SCORE_THRESHOLD):\n",
    "                    confidences.append(confidence)\n",
    "                    class_ids.append(class_id)\n",
    "                    cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "                    left = int((cx - w/2) * x_factor)\n",
    "                    top = int((cy - h/2) * y_factor)\n",
    "                    width = int(w * x_factor)\n",
    "                    height = int(h * y_factor)\n",
    "                    box = np.array([left, top, width, height])\n",
    "                    boxes.append(box)\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant, overlapping boxes with lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    for i in indices:\n",
    "          box = boxes[i]\n",
    "          left = box[0]\n",
    "          top = box[1]\n",
    "          width = box[2]\n",
    "          height = box[3]             \n",
    "          # Draw bounding box.             \n",
    "          cv2.rectangle(input_image, (left, top), (left + width, top + height), BLUE, 3*THICKNESS)\n",
    "          # Save class labels and confidence\n",
    "          labels_confidence.append(confidences[i])\n",
    "          labels_bbox.append([left, top, width, height])\n",
    "          labels_class.append(str(classes[class_ids[i]]))\n",
    "          # Class label.              \n",
    "          label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])             \n",
    "          # Draw label.             \n",
    "          draw_label(input_image, label, left, top)\n",
    "    return labels_class, labels_confidence, labels_bbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(input_image_path, weights_path):\n",
    "\n",
    "    count = 1\n",
    "    all_image_metadata = {}\n",
    "    classesFile = \"coco.names\"\n",
    "    classes = None\n",
    "    with open(classesFile, 'rt') as f:\n",
    "        classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "    frame = cv2.imread(input_image_path)\n",
    "    modelWeights = weights_path #\"/home/samuel/Downloads/yolov5m.onnx\"\n",
    "    net = cv2.dnn.readNet(modelWeights)\n",
    "    \n",
    "    # Process image.\n",
    "    detections = pre_process(frame, net)\n",
    "    img_detections = post_process(frame.copy(), detections)\n",
    "\n",
    "    # Perform face recognition to get face locations\n",
    "    image = face_recognition.load_image_file(input_image_path)\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    \n",
    "    # Get detection data from model\n",
    "    labels_class, labels_confidence, labels_bbox = img_detections\n",
    "\n",
    "    if len(labels_class) != len(face_locations):    \n",
    "        # i.e there are some objects except from a person without face\n",
    "        diff = len(labels_class) - len(face_locations)\n",
    "        n = np.append([], np.repeat(\"Null\", diff))\n",
    "        face_locations = [*face_locations, *n]\n",
    "\n",
    "    print (face_locations)\n",
    "    for i in zip(labels_class, labels_confidence, labels_bbox, face_locations):\n",
    "        label = i[0]\n",
    "        confidence = i[1]\n",
    "        object_bbox = i[2]\n",
    "        face_bbox = i[3]\n",
    "\n",
    "        image_metadata = {\n",
    "            \"label\": label,\n",
    "            \"confidence\": confidence,\n",
    "            \"object_bbox\": object_bbox,\n",
    "        }\n",
    "\n",
    "        face_metadata = {\n",
    "            \"face_bbox\": face_bbox\n",
    "        }\n",
    "\n",
    "        \n",
    "        if face_bbox != \"Null\":\n",
    "            all_image_metadata.update(\n",
    "                {\n",
    "                    \"object_metadata_\" + str(count) : image_metadata,\n",
    "                    \"face_dict_\" + str(count) : face_metadata\n",
    "                })\n",
    "\n",
    "           #print (all_image_metadata)\n",
    "            count += 1\n",
    "        else:\n",
    "            all_image_metadata.update(\n",
    "                {\"object_metadata_\" + str(count) : image_metadata})\n",
    "            count += 1\n",
    "\n",
    "    return all_image_metadata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(66, 322, 129, 259), (73, 460, 135, 398), (72, 220, 146, 146)]\n"
     ]
    }
   ],
   "source": [
    "f_metadata = run_all(\n",
    "    input_image_path=\"/home/samuel/Downloads/istockphoto-638494402-612x612.jpg\",\n",
    "    weights_path=\"/home/samuel/Downloads/yolov5m.onnx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object_metadata_1': {'label': 'person',\n",
       "  'confidence': 0.9281701,\n",
       "  'object_bbox': [369, 46, 222, 361]},\n",
       " 'face_dict_1': {'face_bbox': (66, 322, 129, 259)},\n",
       " 'object_metadata_2': {'label': 'person',\n",
       "  'confidence': 0.9007219,\n",
       "  'object_bbox': [29, 44, 197, 362]},\n",
       " 'face_dict_2': {'face_bbox': (73, 460, 135, 398)},\n",
       " 'object_metadata_3': {'label': 'person',\n",
       "  'confidence': 0.84347767,\n",
       "  'object_bbox': [182, 43, 227, 363]},\n",
       " 'face_dict_3': {'face_bbox': (72, 220, 146, 146)}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_path=\"/home/samuel/Downloads/istockphoto-638494402-612x612.jpg\"\n",
    "image = face_recognition.load_image_file(input_image_path)\n",
    "face_locations = face_recognition.face_locations(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(66, 322, 129, 259), (73, 460, 135, 398), (72, 220, 146, 146)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" t, _ = net.getPerfProfile()\\nlabel = 'Inference time: %.2f ms' % (t * 1000.0 /  cv2.getTickFrequency())\\nprint(label)\\ncv2.putText(img, label, (20, 40), FONT_FACE, FONT_SCALE,  (0, 0, 255), THICKNESS, cv2.LINE_AA)\\ncv2.imshow('Output', img)\\ncv2.waitKey(0) \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if __name__ == '__main__':\n",
    "# Load class names.\n",
    "\n",
    "\"\"\"\n",
    "Put efficiency information. The function getPerfProfile returns       the overall time for inference(t) \n",
    "and the timings for each of the layers(in layersTimes).\n",
    "\"\"\"\n",
    "\"\"\" t, _ = net.getPerfProfile()\n",
    "label = 'Inference time: %.2f ms' % (t * 1000.0 /  cv2.getTickFrequency())\n",
    "print(label)\n",
    "cv2.putText(img, label, (20, 40), FONT_FACE, FONT_SCALE,  (0, 0, 255), THICKNESS, cv2.LINE_AA)\n",
    "cv2.imshow('Output', img)\n",
    "cv2.waitKey(0) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    image = face_recognition.load_image_file(input_image_path)\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    # Get detection data from model\n",
    "    labels_class, labels_confidence, labels_bbox = run_object_detection(\n",
    "        input_image_path = input_image_path)\n",
    "\n",
    "\n",
    "\n",
    "    if len(labels_class) != len(face_locations):    \n",
    "        # i.e there are some objects except from a person without face\n",
    "        diff = len(labels_class) - len(face_locations)\n",
    "        n = np.append([], np.repeat(\"Null\", diff))\n",
    "        face_locations = [*face_locations, *n]\n",
    "\n",
    "    for i in zip(labels_class, labels_confidence, labels_bbox, face_locations):\n",
    "        label = i[0]\n",
    "        confidence = i[1]\n",
    "        object_bbox = i[2]\n",
    "        face_bbox = i[3]\n",
    "\n",
    "        image_metadata = {\n",
    "            \"label\": label,\n",
    "            \"confidence\": confidence,\n",
    "            \"object_bbox\": object_bbox,\n",
    "        }\n",
    "\n",
    "        face_metadata = {\n",
    "            \"face_bbox\": face_bbox\n",
    "        }\n",
    "\n",
    "        if face_bbox != \"Null\":\n",
    "            all_image_metadata.update(\n",
    "                {\n",
    "                    \"object_metadata_\" + str(count) : image_metadata,\n",
    "                    \"face_dict\": face_metadata\n",
    "                })\n",
    "            count += 1\n",
    "        else:\n",
    "            all_image_metadata.update(\n",
    "                {\"object_metadata_\" + str(count) : image_metadata})\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[369, 46, 222, 361], [29, 44, 197, 362], [182, 43, 227, 363]]\n"
     ]
    }
   ],
   "source": [
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "146558185431bdaf94a45b8e6b5403df7057acc22dc3943478a534d154c6584f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('object_detection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
